{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Final Project: Automated solutions for algorithmic bias, Python Portion\n",
    "##### by: Cheng-Yu (Ben) Chiang from MATH 157 Winter 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Resource used: \n",
    "- https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G\n",
    "- https://en.wikipedia.org/wiki/Disparate_impact#:~:text=Disparate%20impact%20in%20United%20States,or%20landlords%20are%20formally%20neutral\n",
    "- https://github.com/Trusted-AI/AIF360\n",
    "- https://dl.acm.org/doi/pdf/10.1145/3278721.3278779\n",
    "- https://aif360.readthedocs.io/en/stable/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360.datasets.BinaryLabelDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## What is algorithmic bias? \n",
    "- Algorithmic bias refers error made by computer algorithms that create unfair outcomes, usually results in privileging one particular group of users over the other.\n",
    "- It can emerge from intentional / unintentional factors such as the design of the algorithm, biased sampling of the data, etc.\n",
    "- Algorithms have a strong social and political impact on our lives: presidential polls, search engines, credit score, advertisement, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example: Amazon's AI recruiting tool is biased against women\n",
    "- One of the examples of algorithmic bias is the famous case of amazon's AI recruiting tool biasing against women. Amazon, like many big tech companies, has been using automated tools to recruit top talents from the huge pool of candidates that they receive applications from. In an ideal situation, all candidates will be judged fairly based off their abilities.\n",
    "- However, in 2015, Amazon discovered that its hiring algorithm for software developers are heavily biased against women. \n",
    "- This is because the machine learning algorithm were trained to observe pattern from previous successful hires and most of these data points come from men.\n",
    "- Eventually, the team that was in charge of this project was disbanded.\n",
    "- Highlights the impact in algorithmic bias and the difficulty in solving such problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The challenge\n",
    "- Bias is a human nature\n",
    "- How can we make sure our algorithms does not amplify our inherent bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data input and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 02:24:44.916791: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-18 02:24:44.916828: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.precision\", 5)\n",
    "\n",
    "\n",
    "col_names = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "             \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "             \"Hours per week\", \"Country\", \"Target\"]\n",
    "\n",
    "train = pd.read_csv(\"raw_dataset/adult.data\", skiprows=0, names=col_names, header=None)\n",
    "test = pd.read_csv(\"raw_dataset/adult.test\", skiprows=1, names=col_names, header=None)\n",
    "\n",
    "df = pd.concat([train, test]).reset_index(drop=True)\n",
    "df = df.drop(['Education', 'fnlwgt', 'Country'], axis=1)\n",
    "df['Sex'] = df['Sex'].str.strip()\n",
    "df['Target'] = df['Target'].str.strip()\n",
    "\n",
    "df['Target'] = df['Target'].replace(['<=50K', '<=50K.'], 0)\n",
    "df['Target'] = df['Target'].replace(['>50K', '>50K.'], 1)\n",
    "df.to_csv(\"parsed_dataset/complete_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Male <=50K\n",
      "count: 22732\n",
      "prob:  0.6962327718223583\n",
      "==============================\n",
      "Male >50K\n",
      "count: 9918\n",
      "prob:  0.3037672281776417\n",
      "==============================\n",
      "Female <=50K\n",
      "count: 14423\n",
      "prob:  0.8907485177865613\n",
      "==============================\n",
      "Female >50K\n",
      "count: 1769\n",
      "prob:  0.10925148221343874\n"
     ]
    }
   ],
   "source": [
    "def summarize(df, gender: str):\n",
    "    gender_df = df[df['Sex'] == gender]\n",
    "    print(\"=\"*30)\n",
    "    print(gender, \"<=50K\")\n",
    "    print(\"count:\", sum(gender_df['Target'] == 0))\n",
    "    print(\"prob: \",  sum(gender_df['Target'] == 0) / len(gender_df))\n",
    "    print(\"=\"*30)\n",
    "    print(gender, \">50K\")\n",
    "    print(\"count:\", sum(gender_df['Target'] == 1))\n",
    "    print(\"prob: \",  sum(gender_df['Target'] == 1) / len(gender_df))\n",
    "    return gender_df\n",
    "     \n",
    "# key should be \"Male\" or \"Female\"\n",
    "male = summarize(df, 'Male')\n",
    "female = summarize(df, 'Female')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As you can see, there are more male represented in the >50k region. Based on the dataset, about 3 in 10 male adult earns more than 50K. On the other hand, only 1 in 10 female adults earn more than 50K. \n",
    "Clearly, something is wrong. Whether or not this is an accurate representation of male and female in the workforce is up for debate. However, as an engineer / scientist our job is to make sure that the algorithm that we designed isn't biased against either group for whatever reason.\n",
    "\n",
    "## Measuring Bias\n",
    "\n",
    "To measure bias, we will not introduce some common metrics. Including Statistical Parity Difference, Equal Opportunity Difference, Average Odds Difference, and Disparate Impact Ratio. Each metric can be interpreted individually or together to give us a insight on whether the dataset / model is biased and in what ways. \n",
    "\n",
    "The first 3 will be introduced in the julia portion of the project and the last metric, disparate impact ratio, will be introduced after this section. \n",
    "\n",
    "#### **If you are reading this presentation instead of being presented to, please head to the Julia portion of this presentation**\n",
    "\n",
    "<br></br>\n",
    "### Disparate impact metric\n",
    "Now, in order to see if our method is working, we need a way to measure the bias in our dataset / population. One good way to do it is using **disparate impact** ratio.\n",
    "\n",
    "Disparate impact ratio is a metric formally defined by the United States labor law by to evaluate fairness based on a predefined positive output. The general equation for the disparate impact is\n",
    "$$\n",
    "\\frac{P(Y=1|\\text{unprivileged})}{P(Y=1|\\text{privileged})}\n",
    "$$\n",
    "We define the positive output as >50K income (Y = 1) and assume that the unprivileged group to be female based on its significantly higher <=50K income probability compared to male. Conversely, we assume that male is the privileged group.\n",
    "\n",
    "So, the disparate impact metric becomes: \n",
    "\n",
    "$$\n",
    "\\frac{P(\\text{income}>\\text{50K}|\\text{women})}{P(\\text{income}>\\text{50K}|\\text{men})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3596552625800337"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(female['Target'] == 1) / len(female)) / (sum(male['Target'] == 1) / len(male))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Solution 1: Reweighting (Preprocessing)\n",
    "One way to mitigate the bias is by modifying the data before the training has taken place.\n",
    "A good method that is commonly used is reweighting. This refers to giving weights to different row features based on their privilege status and ourcome. \n",
    "\n",
    "Below, we will layout the foundation of such method by hand calculating the weights that should be given to different classes / outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The 4/5 Rule\n",
    "The typical standard for an acceptable disparate impact rate that is the four-fifths rule:\n",
    "- if the unprivileged group gets a positive outcome (income > 50K) less than 80% of their proportion of the privileged group, then this counts as a disparate impact violation\n",
    "- this means that our calculation results shown above should be > 0.8 for it to not be a violation\n",
    "- our disparate impact ratio of `0.358` means that there is clearly an bias against women in terms of income based on the sampling in this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Risk of Training with Biased data\n",
    "\n",
    "The risk of training with biased data lies in the possibility of the model amplifying the mistake. \n",
    "\n",
    "As a machine learning model, it's goal is to maximum its accuracy. Therefore, it will be naturally intended to lean towards the privileged group. \n",
    "\n",
    "An example to understand this is: \n",
    "- say we have a dataset with 10000 samples, 1000 of them has gender = female and the rest is marked male.\n",
    "- If our model were to predict with 0.5 probability (randomly predicting), then it's expected accuracy is only 0.9 * 0.5 + 0.1 * 0.5 = 0.5 (50%)\n",
    "- However, if our model predicts male all the time, then it will be right 90% of the time. \n",
    "- Therefore, the model is inclined to be biased towards male simply because it is more well represented in the dataset. \n",
    "\n",
    "Below we provide a real example of how classifying using biased data can lead to worse biasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_df = pd.get_dummies(df)\n",
    "labels = categorical_df['Target']\n",
    "features = categorical_df.drop(['Target'], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_df, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LogisticRegression()\n",
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy of the model is: 0.85006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>class_0_prob</th>\n",
       "      <th>class_1_prob</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7762</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99531</td>\n",
       "      <td>0.00469</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23881</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99826</td>\n",
       "      <td>0.00174</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30507</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99665</td>\n",
       "      <td>0.00335</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28911</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99555</td>\n",
       "      <td>0.00445</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19484</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.97298</td>\n",
       "      <td>0.02702</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target  prediction  class_0_prob  class_1_prob  gender\n",
       "7762        0           0       0.99531       0.00469    Male\n",
       "23881       0           0       0.99826       0.00174  Female\n",
       "30507       0           0       0.99665       0.00335    Male\n",
       "28911       0           0       0.99555       0.00445  Female\n",
       "19484       0           0       0.97298       0.02702    Male"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model(x, y, df):\n",
    "    orig_matching_df = df.iloc[x.index, :]\n",
    "    pred = reg.predict(x)\n",
    "    probs = reg.predict_proba(x)\n",
    "    print(\"The mean accuracy of the model is:\", round(reg.score(x, y), 5))\n",
    "    data = {'target': y, 'prediction': pred ,'class_0_prob': probs[:, 0], 'class_1_prob': probs[:, 1], 'gender': orig_matching_df['Sex']}\n",
    "    result_df = pd.DataFrame(data)\n",
    "    return result_df\n",
    "\n",
    "result_df = test_model(x_test, y_test, df)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicts 7.034 % of female earn >50K\n",
      "Model predicts 25.176 % of male earn >50K\n",
      "Predicted disparate impact ratio is: 0.27939670399997324\n"
     ]
    }
   ],
   "source": [
    "female = result_df[result_df['gender'] == 'Female']\n",
    "print('Model predicts', round(sum(female['prediction'] == 1) / len(female), 5) * 100, '% of female earn >50K')\n",
    "male = result_df[result_df['gender'] == 'Male']\n",
    "print('Model predicts', round(sum(male['prediction'] == 1) / len(male), 5) * 100, '% of male earn >50K')\n",
    "\n",
    "print(\"Predicted disparate impact ratio is:\", (sum(female['prediction'] == 1) / len(female)) / (sum(male['prediction'] == 1) / len(male)))\n",
    "\n",
    "result_df.to_csv(\"parsed_dataset/biased_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Clearly, the classifier has worsened the bias in the dataset. It will be critical that we mitigate this before using it in any way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assigning weights to different classes\n",
    "To rectify this bias what we can do is apply a weight for each class depending on whether it its unprivileged and privileged. Intuitively, we would want to give the privileged class (male) less weighting and the less privileged class (female) a heavier weight. \n",
    "\n",
    "This can be formulated as the following equation: \n",
    "$$\n",
    "W = \\frac{N_{\\text{unprivileged}}N_{\\text{positive}}}{N_{\\text{all}}N_{\\text{positive unprivileged}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight for positive underprivileged class: 2.19019\n",
      "weight for negative underprivileged class: 0.85402\n",
      "weight for position privileged class: 0.78771\n",
      "weight for negative privileged class: 1.09262\n"
     ]
    }
   ],
   "source": [
    "male = df[df['Sex'] == 'Male']\n",
    "female = df[df['Sex'] == 'Female']\n",
    "\n",
    "pos_unpri = (len(female) * sum(df['Target'] == 1)) / (len(df) * sum(female['Target'] == 1))\n",
    "neg_unpri = (len(female) * sum(df['Target'] == 0)) / (len(df) * sum(female['Target'] == 0))\n",
    "\n",
    "pos_pri = (len(male) * sum(df['Target'] == 1)) / (len(df) * sum(male['Target'] == 1))\n",
    "neg_pri = (len(male) * sum(df['Target'] == 0)) / (len(df) * sum(male['Target'] == 0))\n",
    "\n",
    "print('weight for positive underprivileged class:', round(pos_unpri, 5))\n",
    "print('weight for negative underprivileged class:', round(neg_unpri, 5))\n",
    "print('weight for position privileged class:', round(pos_pri, 5))\n",
    "print('weight for negative privileged class:', round(neg_pri, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How can we explain these results intuitively?\n",
    "- higher weight for positive underprivileged class: we want to boost the positive features of underprivileged class\n",
    "- lower weight for negative underprivileged class: we want to reduce the negative features of underprivileged class\n",
    "- lower weight for positive privileged class: we want to reduce the amount of positive for the privileged class\n",
    "- slightly higher weight for negative privileged class: we want to slightly increase the effect of negative attributes to the privileged class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reweighing the dataset and retrain the model \n",
    "Now, after we obtained the weights, what we want to do is to apply the calculated weights to the entire dataset. To implementing this, we can simply use the provided function from `aif360` library `Reweighing`. \n",
    "A brief introduction to the `aif360` library: \n",
    "- Developed by IBM's research team Trusted AI, it is an opensource toolkit for examining, reporting, and mitigating discrimination and bias in machine learning models.\n",
    "- The package includes comprehensive set of metrics for biases and models and a collection of algorithms to mitigate bias in dataset and machine learned models. \n",
    "- Using the package, we can load common datasets such as the Adult Census Income Dataset, Bank marketing Dataset, German credit Dataset, etc.\n",
    "- The package splits the algorithms into 3 main types:\n",
    "    1. Preprocessing: modifying the dataset before training\n",
    "    2. Inprocessing: modifying the training process to ensure fairness\n",
    "    3. Postprocessing: calibrate, reject, or equalize outcomes after training has been done\n",
    "- In this example, I will only demonstrate a few examples of the functionality that it provides. To learn more, go to https://aif360.mybluemix.net/data for an interactive demo\n",
    "\n",
    "Reweighting is an algorithm that belongs to the class of preprocessing algorithms because it modifies the dataset before training happens.\n",
    "\n",
    "Below, we demonstrate how `Reweighing` can be used to mitigate bias in the dataset and make the model less prone to be biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take out gender column so it doesn't get transformed\n",
    "gender_df = df['Sex'].copy()\n",
    "gender_df[gender_df == 'Female'] = 1\n",
    "gender_df[gender_df == 'Male'] = 0\n",
    "\n",
    "# drop the unnecessary columns\n",
    "new_df = pd.get_dummies(df).drop(['Sex_Male', 'Sex_Female'], axis=1)\n",
    "new_df['Gender'] = gender_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.09262055 1.09262055 1.09262055 ... 1.09262055 1.09262055 0.78771422] \n",
      " 48842\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1.09262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1.09262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1.09262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1.09262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>2.19019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0.85402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1.09262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.78771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gender  target   weight\n",
       "0     Male       0  1.09262\n",
       "1     Male       0  1.09262\n",
       "2     Male       0  1.09262\n",
       "3     Male       0  1.09262\n",
       "4   Female       0  0.85402\n",
       "5   Female       0  0.85402\n",
       "6   Female       0  0.85402\n",
       "7     Male       1  0.78771\n",
       "8   Female       1  2.19019\n",
       "9     Male       1  0.78771\n",
       "10    Male       1  0.78771\n",
       "11    Male       1  0.78771\n",
       "12  Female       0  0.85402\n",
       "13    Male       0  1.09262\n",
       "14    Male       1  0.78771"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert encoded data into required aif format\n",
    "BLD = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=new_df, label_names=['Target'], protected_attribute_names=['Gender'])\n",
    "\n",
    "privileged_groups = [{'Gender': 1}]\n",
    "unprivileged_groups = [{'Gender': 0}]\n",
    "\n",
    "# reweight the dataset using built-in function from aif360\n",
    "RW = Reweighing(unprivileged_groups=[{'Gender': 0}], privileged_groups=[{'Gender': 1}])\n",
    "di = RW.fit_transform(BLD)\n",
    "print(di.instance_weights, \"\\n\", len(di.instance_weights))\n",
    "\n",
    "data = {'gender': df['Sex'], 'target': new_df['Target'], 'weight': di.instance_weights}\n",
    "pd.DataFrame(data).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we can see here, the previous weights that we calculated are now applied to the entire dataset based on which of the four classes that it is in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "di_features = pd.DataFrame(di.features, columns=di.feature_names).reset_index(drop=True)\n",
    "\n",
    "# scale the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = pd.DataFrame(scaler.fit_transform(di_features), columns=di_features.columns)\n",
    "\n",
    "# split the dataset into train / test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_features, di.labels.flatten(), test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(x_train, y_train, sample_weight=di.instance_weights[x_train.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy of the model is: 0.84324\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>class_0_prob</th>\n",
       "      <th>class_1_prob</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7762</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99651</td>\n",
       "      <td>0.00349</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23881</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99573</td>\n",
       "      <td>0.00427</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30507</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99773</td>\n",
       "      <td>0.00227</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28911</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98880</td>\n",
       "      <td>0.01120</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19484</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98053</td>\n",
       "      <td>0.01947</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15938</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65240</td>\n",
       "      <td>0.34760</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27828</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.99038</td>\n",
       "      <td>0.00962</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28449</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94207</td>\n",
       "      <td>0.05793</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5647</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98255</td>\n",
       "      <td>0.01745</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27058</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63564</td>\n",
       "      <td>0.36436</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14653 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target  prediction  class_0_prob  class_1_prob  gender\n",
       "7762      0.0         0.0       0.99651       0.00349    Male\n",
       "23881     0.0         0.0       0.99573       0.00427  Female\n",
       "30507     0.0         0.0       0.99773       0.00227    Male\n",
       "28911     0.0         0.0       0.98880       0.01120  Female\n",
       "19484     0.0         0.0       0.98053       0.01947    Male\n",
       "...       ...         ...           ...           ...     ...\n",
       "15938     0.0         0.0       0.65240       0.34760    Male\n",
       "27828     0.0         0.0       0.99038       0.00962  Female\n",
       "28449     0.0         0.0       0.94207       0.05793  Female\n",
       "5647      0.0         0.0       0.98255       0.01745    Male\n",
       "27058     0.0         0.0       0.63564       0.36436  Female\n",
       "\n",
       "[14653 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = test_model(x_test, y_test, df)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicts 12.073 % of female earn >50K\n",
      "Model predicts 20.488 % of male earn >50K\n",
      "Predicted disparate impact ratio is: 0.5892766989037003\n"
     ]
    }
   ],
   "source": [
    "female = result_df[result_df['gender'] == 'Female']\n",
    "print('Model predicts', round(sum(female['prediction'] == 1) / len(female), 5) * 100, '% of female earn >50K')\n",
    "\n",
    "male = result_df[result_df['gender'] == 'Male']\n",
    "print('Model predicts', round(sum(male['prediction'] == 1) / len(male), 5) * 100, '% of male earn >50K')\n",
    "\n",
    "print(\"Predicted disparate impact ratio is:\", (sum(female['prediction'] == 1) / len(female)) / (sum(male['prediction'] == 1) / len(male)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Solution 2: Adversarial Debiasing\n",
    "- An inprocessing technique which learns two classifiers in simultaneously to ensure that protected attributes such as gender and race cannot be used to produce biased models. \n",
    "- One classifier's job is to maximize prediction accuracy or minimize loss\n",
    "- The other classifier tries to determine the protected attribute from the prediction produced by the first classifier. \n",
    "- Goal: to minimize the adversary's ability to correctly identify the protected attribute while reducing the error of the original classification problem. \n",
    "- Intuitively: if the difference in results between unprivileged and privileged group cannot be discerned, then we have achieved our goal of removing bias from the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 02:25:01.762683: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-18 02:25:01.763252: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-18 02:25:01.763786: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (project-38c1918c-f051-4428-8847-997d00b4b0c8): /proc/driver/nvidia/version does not exist\n",
      "2022-03-18 02:25:01.771621: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_df = df[['Age', 'Workclass', 'Education-Num', 'Occupation', 'Sex', 'Capital Gain', 'Capital Loss',\n",
    "       'Hours per week', 'Target']]\n",
    "\n",
    "gender_df = filtered_df['Sex'].copy()\n",
    "gender_df[gender_df == 'Female'] = 1\n",
    "gender_df[gender_df == 'Male'] = 0\n",
    "\n",
    "# drop the unnecessary columns\n",
    "new_df = pd.get_dummies(filtered_df).drop(['Sex_Male', 'Sex_Female'], axis=1)\n",
    "new_df['Gender'] = gender_df\n",
    "new_df = new_df.drop(\"Workclass_ ?\", axis=1)\n",
    "\n",
    "\n",
    "BLD = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=new_df, label_names=['Target'], protected_attribute_names=['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "privileged_groups = [{'Gender': 1}]\n",
    "unprivileged_groups = [{'Gender': 0}]\n",
    "\n",
    "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess,\n",
    "                          num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 26.520050; batch adversarial loss: 0.721980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 200; batch classifier loss: 6.575389; batch adversarial loss: 0.672951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1; iter: 0; batch classifier loss: 15.754431; batch adversarial loss: 0.648825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1; iter: 200; batch classifier loss: 8.892447; batch adversarial loss: 0.636596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2; iter: 0; batch classifier loss: 8.706734; batch adversarial loss: 0.633119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2; iter: 200; batch classifier loss: 3.740513; batch adversarial loss: 0.678879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3; iter: 0; batch classifier loss: 0.936135; batch adversarial loss: 0.632686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3; iter: 200; batch classifier loss: 1.568327; batch adversarial loss: 0.635623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4; iter: 0; batch classifier loss: 0.520397; batch adversarial loss: 0.619828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4; iter: 200; batch classifier loss: 0.564673; batch adversarial loss: 0.565578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5; iter: 0; batch classifier loss: 1.424243; batch adversarial loss: 0.658711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5; iter: 200; batch classifier loss: 1.433200; batch adversarial loss: 0.639733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6; iter: 0; batch classifier loss: 0.442910; batch adversarial loss: 0.620846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6; iter: 200; batch classifier loss: 0.554492; batch adversarial loss: 0.654188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7; iter: 0; batch classifier loss: 0.827659; batch adversarial loss: 0.634674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7; iter: 200; batch classifier loss: 0.513500; batch adversarial loss: 0.610400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8; iter: 0; batch classifier loss: 0.491024; batch adversarial loss: 0.658965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8; iter: 200; batch classifier loss: 0.514090; batch adversarial loss: 0.622027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9; iter: 0; batch classifier loss: 0.550617; batch adversarial loss: 0.614941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9; iter: 200; batch classifier loss: 0.599395; batch adversarial loss: 0.628909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10; iter: 0; batch classifier loss: 0.478407; batch adversarial loss: 0.703508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10; iter: 200; batch classifier loss: 0.460160; batch adversarial loss: 0.582949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11; iter: 0; batch classifier loss: 0.444950; batch adversarial loss: 0.621784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11; iter: 200; batch classifier loss: 0.330203; batch adversarial loss: 0.613746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12; iter: 0; batch classifier loss: 0.493532; batch adversarial loss: 0.577208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12; iter: 200; batch classifier loss: 0.373200; batch adversarial loss: 0.654899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13; iter: 0; batch classifier loss: 0.480373; batch adversarial loss: 0.668941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13; iter: 200; batch classifier loss: 0.434231; batch adversarial loss: 0.653616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14; iter: 0; batch classifier loss: 0.457155; batch adversarial loss: 0.596422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14; iter: 200; batch classifier loss: 0.451609; batch adversarial loss: 0.619495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15; iter: 0; batch classifier loss: 0.441263; batch adversarial loss: 0.616872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15; iter: 200; batch classifier loss: 0.360725; batch adversarial loss: 0.591028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16; iter: 0; batch classifier loss: 0.478597; batch adversarial loss: 0.646450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16; iter: 200; batch classifier loss: 0.331256; batch adversarial loss: 0.633863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17; iter: 0; batch classifier loss: 0.373736; batch adversarial loss: 0.634879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17; iter: 200; batch classifier loss: 0.378821; batch adversarial loss: 0.629062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18; iter: 0; batch classifier loss: 0.483393; batch adversarial loss: 0.618329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18; iter: 200; batch classifier loss: 0.442795; batch adversarial loss: 0.558951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19; iter: 0; batch classifier loss: 0.493609; batch adversarial loss: 0.518423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19; iter: 200; batch classifier loss: 0.397547; batch adversarial loss: 0.555116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20; iter: 0; batch classifier loss: 0.337871; batch adversarial loss: 0.630228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20; iter: 200; batch classifier loss: 0.508213; batch adversarial loss: 0.559340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21; iter: 0; batch classifier loss: 0.434960; batch adversarial loss: 0.607187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21; iter: 200; batch classifier loss: 0.384430; batch adversarial loss: 0.602530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22; iter: 0; batch classifier loss: 0.318251; batch adversarial loss: 0.644566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22; iter: 200; batch classifier loss: 0.398346; batch adversarial loss: 0.638102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23; iter: 0; batch classifier loss: 0.457130; batch adversarial loss: 0.686689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23; iter: 200; batch classifier loss: 0.435053; batch adversarial loss: 0.637149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24; iter: 0; batch classifier loss: 0.359426; batch adversarial loss: 0.621873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24; iter: 200; batch classifier loss: 0.407546; batch adversarial loss: 0.620361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25; iter: 0; batch classifier loss: 0.444263; batch adversarial loss: 0.629434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25; iter: 200; batch classifier loss: 0.448907; batch adversarial loss: 0.609631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26; iter: 0; batch classifier loss: 0.465315; batch adversarial loss: 0.663206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26; iter: 200; batch classifier loss: 0.389895; batch adversarial loss: 0.650545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27; iter: 0; batch classifier loss: 0.363650; batch adversarial loss: 0.645354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27; iter: 200; batch classifier loss: 0.323745; batch adversarial loss: 0.648873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28; iter: 0; batch classifier loss: 0.402512; batch adversarial loss: 0.605798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28; iter: 200; batch classifier loss: 0.325530; batch adversarial loss: 0.548896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29; iter: 0; batch classifier loss: 0.398492; batch adversarial loss: 0.618267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29; iter: 200; batch classifier loss: 0.336571; batch adversarial loss: 0.598027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30; iter: 0; batch classifier loss: 0.412983; batch adversarial loss: 0.617439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30; iter: 200; batch classifier loss: 0.799667; batch adversarial loss: 0.586326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31; iter: 0; batch classifier loss: 0.355098; batch adversarial loss: 0.559492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31; iter: 200; batch classifier loss: 0.494161; batch adversarial loss: 0.619415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32; iter: 0; batch classifier loss: 0.378052; batch adversarial loss: 0.604950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32; iter: 200; batch classifier loss: 0.457935; batch adversarial loss: 0.616417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33; iter: 0; batch classifier loss: 0.480134; batch adversarial loss: 0.602581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 33; iter: 200; batch classifier loss: 0.390843; batch adversarial loss: 0.633257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34; iter: 0; batch classifier loss: 0.400778; batch adversarial loss: 0.663468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34; iter: 200; batch classifier loss: 0.380937; batch adversarial loss: 0.602046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35; iter: 0; batch classifier loss: 0.371457; batch adversarial loss: 0.668501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 35; iter: 200; batch classifier loss: 0.330504; batch adversarial loss: 0.568554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36; iter: 0; batch classifier loss: 0.468557; batch adversarial loss: 0.592391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36; iter: 200; batch classifier loss: 0.327116; batch adversarial loss: 0.696013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37; iter: 0; batch classifier loss: 0.479114; batch adversarial loss: 0.604080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 37; iter: 200; batch classifier loss: 0.533110; batch adversarial loss: 0.603514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38; iter: 0; batch classifier loss: 0.424274; batch adversarial loss: 0.592869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38; iter: 200; batch classifier loss: 0.381855; batch adversarial loss: 0.562479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39; iter: 0; batch classifier loss: 0.393235; batch adversarial loss: 0.644126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 39; iter: 200; batch classifier loss: 0.544316; batch adversarial loss: 0.609787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40; iter: 0; batch classifier loss: 0.411553; batch adversarial loss: 0.621930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40; iter: 200; batch classifier loss: 0.716560; batch adversarial loss: 0.600227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41; iter: 0; batch classifier loss: 0.357922; batch adversarial loss: 0.616588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41; iter: 200; batch classifier loss: 0.404680; batch adversarial loss: 0.606049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42; iter: 0; batch classifier loss: 0.582821; batch adversarial loss: 0.670913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42; iter: 200; batch classifier loss: 0.561658; batch adversarial loss: 0.606481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43; iter: 0; batch classifier loss: 0.408466; batch adversarial loss: 0.619658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 43; iter: 200; batch classifier loss: 0.500393; batch adversarial loss: 0.654502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44; iter: 0; batch classifier loss: 0.426844; batch adversarial loss: 0.619151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44; iter: 200; batch classifier loss: 0.296007; batch adversarial loss: 0.632768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45; iter: 0; batch classifier loss: 0.359848; batch adversarial loss: 0.573729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 45; iter: 200; batch classifier loss: 0.468365; batch adversarial loss: 0.676520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46; iter: 0; batch classifier loss: 0.565396; batch adversarial loss: 0.604404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 46; iter: 200; batch classifier loss: 0.527489; batch adversarial loss: 0.621284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47; iter: 0; batch classifier loss: 0.437843; batch adversarial loss: 0.557883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 47; iter: 200; batch classifier loss: 0.437752; batch adversarial loss: 0.557434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48; iter: 0; batch classifier loss: 0.521939; batch adversarial loss: 0.646541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 48; iter: 200; batch classifier loss: 0.414165; batch adversarial loss: 0.668098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49; iter: 0; batch classifier loss: 0.382521; batch adversarial loss: 0.590477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49; iter: 200; batch classifier loss: 0.523422; batch adversarial loss: 0.623466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50; iter: 0; batch classifier loss: 0.379800; batch adversarial loss: 0.677158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50; iter: 200; batch classifier loss: 0.581437; batch adversarial loss: 0.609218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51; iter: 0; batch classifier loss: 0.460286; batch adversarial loss: 0.552050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 51; iter: 200; batch classifier loss: 0.585025; batch adversarial loss: 0.640073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52; iter: 0; batch classifier loss: 0.440576; batch adversarial loss: 0.620362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 52; iter: 200; batch classifier loss: 0.500468; batch adversarial loss: 0.615953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53; iter: 0; batch classifier loss: 0.548903; batch adversarial loss: 0.611425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53; iter: 200; batch classifier loss: 0.466510; batch adversarial loss: 0.576406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54; iter: 0; batch classifier loss: 0.506385; batch adversarial loss: 0.640013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54; iter: 200; batch classifier loss: 0.475267; batch adversarial loss: 0.560353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55; iter: 0; batch classifier loss: 0.477082; batch adversarial loss: 0.609644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55; iter: 200; batch classifier loss: 0.705489; batch adversarial loss: 0.585191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56; iter: 0; batch classifier loss: 0.344180; batch adversarial loss: 0.639899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56; iter: 200; batch classifier loss: 0.516345; batch adversarial loss: 0.605311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57; iter: 0; batch classifier loss: 1.051936; batch adversarial loss: 0.644423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 57; iter: 200; batch classifier loss: 0.323015; batch adversarial loss: 0.627936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58; iter: 0; batch classifier loss: 0.433234; batch adversarial loss: 0.625571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 58; iter: 200; batch classifier loss: 0.375474; batch adversarial loss: 0.658097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59; iter: 0; batch classifier loss: 0.628997; batch adversarial loss: 0.565198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 59; iter: 200; batch classifier loss: 0.451645; batch adversarial loss: 0.617592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60; iter: 0; batch classifier loss: 0.404796; batch adversarial loss: 0.630170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60; iter: 200; batch classifier loss: 0.582468; batch adversarial loss: 0.619873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61; iter: 0; batch classifier loss: 0.411227; batch adversarial loss: 0.596337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 61; iter: 200; batch classifier loss: 0.420566; batch adversarial loss: 0.619979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62; iter: 0; batch classifier loss: 0.410043; batch adversarial loss: 0.642399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62; iter: 200; batch classifier loss: 0.262852; batch adversarial loss: 0.587193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63; iter: 0; batch classifier loss: 0.573976; batch adversarial loss: 0.615368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63; iter: 200; batch classifier loss: 0.608135; batch adversarial loss: 0.613981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64; iter: 0; batch classifier loss: 0.538630; batch adversarial loss: 0.564292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 64; iter: 200; batch classifier loss: 0.471639; batch adversarial loss: 0.592460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65; iter: 0; batch classifier loss: 0.454550; batch adversarial loss: 0.600444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 65; iter: 200; batch classifier loss: 0.539123; batch adversarial loss: 0.618549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66; iter: 0; batch classifier loss: 0.548452; batch adversarial loss: 0.667517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66; iter: 200; batch classifier loss: 0.494669; batch adversarial loss: 0.627778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67; iter: 0; batch classifier loss: 0.479128; batch adversarial loss: 0.636439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67; iter: 200; batch classifier loss: 0.456680; batch adversarial loss: 0.635043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68; iter: 0; batch classifier loss: 0.392934; batch adversarial loss: 0.647577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68; iter: 200; batch classifier loss: 0.447730; batch adversarial loss: 0.583593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69; iter: 0; batch classifier loss: 0.313820; batch adversarial loss: 0.572590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69; iter: 200; batch classifier loss: 0.484785; batch adversarial loss: 0.621506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70; iter: 0; batch classifier loss: 0.441307; batch adversarial loss: 0.580799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70; iter: 200; batch classifier loss: 0.557256; batch adversarial loss: 0.595662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71; iter: 0; batch classifier loss: 0.400243; batch adversarial loss: 0.641795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71; iter: 200; batch classifier loss: 0.476855; batch adversarial loss: 0.611976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72; iter: 0; batch classifier loss: 0.488860; batch adversarial loss: 0.641287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72; iter: 200; batch classifier loss: 0.491863; batch adversarial loss: 0.602648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73; iter: 0; batch classifier loss: 0.492133; batch adversarial loss: 0.611998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73; iter: 200; batch classifier loss: 0.389102; batch adversarial loss: 0.603767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74; iter: 0; batch classifier loss: 0.598418; batch adversarial loss: 0.688751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 74; iter: 200; batch classifier loss: 0.412226; batch adversarial loss: 0.630671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75; iter: 0; batch classifier loss: 0.463557; batch adversarial loss: 0.555454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75; iter: 200; batch classifier loss: 0.550895; batch adversarial loss: 0.650518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 76; iter: 0; batch classifier loss: 0.620233; batch adversarial loss: 0.599568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 76; iter: 200; batch classifier loss: 0.502021; batch adversarial loss: 0.580248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77; iter: 0; batch classifier loss: 0.509107; batch adversarial loss: 0.611209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77; iter: 200; batch classifier loss: 0.644740; batch adversarial loss: 0.640341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 78; iter: 0; batch classifier loss: 0.615663; batch adversarial loss: 0.606928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 78; iter: 200; batch classifier loss: 0.455263; batch adversarial loss: 0.622632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79; iter: 0; batch classifier loss: 0.485389; batch adversarial loss: 0.636164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79; iter: 200; batch classifier loss: 0.392409; batch adversarial loss: 0.582776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80; iter: 0; batch classifier loss: 0.436945; batch adversarial loss: 0.705351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 80; iter: 200; batch classifier loss: 0.410058; batch adversarial loss: 0.602996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81; iter: 0; batch classifier loss: 0.572287; batch adversarial loss: 0.627186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81; iter: 200; batch classifier loss: 0.339898; batch adversarial loss: 0.581311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82; iter: 0; batch classifier loss: 0.488578; batch adversarial loss: 0.661467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 82; iter: 200; batch classifier loss: 0.385276; batch adversarial loss: 0.633955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83; iter: 0; batch classifier loss: 0.447498; batch adversarial loss: 0.621476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83; iter: 200; batch classifier loss: 0.434678; batch adversarial loss: 0.616745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 84; iter: 0; batch classifier loss: 0.590795; batch adversarial loss: 0.593546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 84; iter: 200; batch classifier loss: 0.486707; batch adversarial loss: 0.547683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85; iter: 0; batch classifier loss: 0.480030; batch adversarial loss: 0.582969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85; iter: 200; batch classifier loss: 0.474519; batch adversarial loss: 0.658126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86; iter: 0; batch classifier loss: 0.457927; batch adversarial loss: 0.654213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 86; iter: 200; batch classifier loss: 0.449076; batch adversarial loss: 0.596764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87; iter: 0; batch classifier loss: 0.377987; batch adversarial loss: 0.626256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87; iter: 200; batch classifier loss: 0.439079; batch adversarial loss: 0.629761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88; iter: 0; batch classifier loss: 0.554024; batch adversarial loss: 0.527594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88; iter: 200; batch classifier loss: 0.434524; batch adversarial loss: 0.661275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89; iter: 0; batch classifier loss: 0.556957; batch adversarial loss: 0.576559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89; iter: 200; batch classifier loss: 0.408172; batch adversarial loss: 0.653879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90; iter: 0; batch classifier loss: 0.602316; batch adversarial loss: 0.610464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90; iter: 200; batch classifier loss: 0.490156; batch adversarial loss: 0.588832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91; iter: 0; batch classifier loss: 0.391693; batch adversarial loss: 0.628557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91; iter: 200; batch classifier loss: 0.466361; batch adversarial loss: 0.586307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92; iter: 0; batch classifier loss: 0.554599; batch adversarial loss: 0.629932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92; iter: 200; batch classifier loss: 0.715748; batch adversarial loss: 0.578237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93; iter: 0; batch classifier loss: 0.429510; batch adversarial loss: 0.642800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93; iter: 200; batch classifier loss: 0.466947; batch adversarial loss: 0.634181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94; iter: 0; batch classifier loss: 0.433265; batch adversarial loss: 0.667963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94; iter: 200; batch classifier loss: 0.591680; batch adversarial loss: 0.625345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95; iter: 0; batch classifier loss: 0.547993; batch adversarial loss: 0.557559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 95; iter: 200; batch classifier loss: 0.614790; batch adversarial loss: 0.561463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96; iter: 0; batch classifier loss: 0.472355; batch adversarial loss: 0.639440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96; iter: 200; batch classifier loss: 0.476184; batch adversarial loss: 0.635030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97; iter: 0; batch classifier loss: 0.701959; batch adversarial loss: 0.653899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 97; iter: 200; batch classifier loss: 0.771629; batch adversarial loss: 0.591383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98; iter: 0; batch classifier loss: 0.501463; batch adversarial loss: 0.580412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98; iter: 200; batch classifier loss: 0.404070; batch adversarial loss: 0.653818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99; iter: 0; batch classifier loss: 0.693034; batch adversarial loss: 0.652979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 99; iter: 200; batch classifier loss: 0.492253; batch adversarial loss: 0.604289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.algorithms.inprocessing.adversarial_debiasing.AdversarialDebiasing at 0x7f6a3c991ca0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debiased_model.fit(BLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can use the built-in functionality of aif360 to measure how our model is performing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.093089\n",
      "Difference in mean outcomes between unprivileged and privileged groups = 0.194516\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "debiased_ds = debiased_model.predict(BLD)\n",
    "metric = BinaryLabelDatasetMetric(debiased_ds, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric.mean_difference())\n",
    "\n",
    "metric = BinaryLabelDatasetMetric(BLD, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can calculate mean difference easily using pandas as well:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19112980418681333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(male['target']) / len(male) - sum(female['target']) / len(female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However, this is rather a crude metric, let's apply some of the metrics that we discussed earlier in the julia script to see how our model performs compared to our previous ones.\n",
    "\n",
    "Thankfully, aif360 has built-in functionality of these metrics, so we don't have to hand write them like before. We can simply use the methods provided in `from aif360.metrics`, specifically `ClassificationMetric` directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8312722656729864\n",
      "0.044071753083786636\n",
      "0.08035729043732376\n"
     ]
    }
   ],
   "source": [
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "classification_metrics = ClassificationMetric(BLD, debiased_ds, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "print(classification_metrics.accuracy())\n",
    "print(classification_metrics.average_odds_difference())\n",
    "print(classification_metrics.equal_opportunity_difference())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
