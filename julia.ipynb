{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Final Project: Automated solutions for algorithmic bias, Julia Portion\n",
    "##### by: Cheng-Yu (Ben) Chiang from MATH 157 Winter 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Limitations of Julia Language\n",
    "Compared to Python, Julia's support for automated solutions for algorithmic bias are more rare and less extensive. Initially, I intended to use `PyCall` and import the related `aif360` functions. However, due to the lack of documentation on aif360's use in Julia I could not successfully import and use the package. After a bit of research, I stumbled across another community package [fairness.jl](https://github.com/ashryaagr/Fairness.jl) but couldn't successfully install the package due to lack of memory (Cocalc terminates my program whenever I try to install it). \n",
    "\n",
    "Therefore, as a workaround, we will explore different metrics of bias by calculating them on the adult dataset to gain a better understanding of the metrics that we will use in our python script later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Firstly, lets load the dataset that was generated from predicting the label of the adult dataset using a standard logistic regression. This dataset contains the target, predicted values, and the gender of the samples. We need to first run a sample prediction because two of the metrics that we are going to discuss here examines the predicted values against the labels and we cannot do that with just the original dataset.\n",
    "Note: the code that generates this data can be found in the python notebook's \"Risk of Training with Biased Data\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Column1</th><th>target</th><th>prediction</th><th>class_0_prob</th><th>class_1_prob</th><th>gender</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Int64</th><th>Float64</th><th>Float64</th><th>String</th></tr></thead><tbody><p>14,653 rows × 6 columns</p><tr><th>1</th><td>7762</td><td>0</td><td>0</td><td>0.995306</td><td>0.00469379</td><td>Male</td></tr><tr><th>2</th><td>23881</td><td>0</td><td>0</td><td>0.998258</td><td>0.00174155</td><td>Female</td></tr><tr><th>3</th><td>30507</td><td>0</td><td>0</td><td>0.996647</td><td>0.00335252</td><td>Male</td></tr><tr><th>4</th><td>28911</td><td>0</td><td>0</td><td>0.995548</td><td>0.0044521</td><td>Female</td></tr><tr><th>5</th><td>19484</td><td>0</td><td>0</td><td>0.972979</td><td>0.027021</td><td>Male</td></tr><tr><th>6</th><td>43031</td><td>0</td><td>0</td><td>0.587811</td><td>0.412189</td><td>Male</td></tr><tr><th>7</th><td>28188</td><td>0</td><td>0</td><td>0.758536</td><td>0.241464</td><td>Male</td></tr><tr><th>8</th><td>12761</td><td>0</td><td>0</td><td>0.896304</td><td>0.103696</td><td>Male</td></tr><tr><th>9</th><td>40834</td><td>0</td><td>0</td><td>0.713358</td><td>0.286642</td><td>Female</td></tr><tr><th>10</th><td>27875</td><td>0</td><td>0</td><td>0.702419</td><td>0.297581</td><td>Female</td></tr><tr><th>11</th><td>1276</td><td>1</td><td>1</td><td>0.157644</td><td>0.842356</td><td>Male</td></tr><tr><th>12</th><td>22608</td><td>1</td><td>0</td><td>0.690165</td><td>0.309835</td><td>Male</td></tr><tr><th>13</th><td>36230</td><td>0</td><td>0</td><td>0.984482</td><td>0.0155177</td><td>Female</td></tr><tr><th>14</th><td>13398</td><td>0</td><td>0</td><td>0.696728</td><td>0.303272</td><td>Male</td></tr><tr><th>15</th><td>43536</td><td>0</td><td>0</td><td>0.96563</td><td>0.0343704</td><td>Male</td></tr><tr><th>16</th><td>18627</td><td>0</td><td>0</td><td>0.726404</td><td>0.273596</td><td>Male</td></tr><tr><th>17</th><td>38424</td><td>0</td><td>0</td><td>0.653893</td><td>0.346107</td><td>Male</td></tr><tr><th>18</th><td>35505</td><td>1</td><td>1</td><td>0.3035</td><td>0.6965</td><td>Male</td></tr><tr><th>19</th><td>2372</td><td>0</td><td>0</td><td>0.760428</td><td>0.239572</td><td>Male</td></tr><tr><th>20</th><td>3375</td><td>0</td><td>0</td><td>0.70922</td><td>0.29078</td><td>Male</td></tr><tr><th>21</th><td>28331</td><td>0</td><td>0</td><td>0.99503</td><td>0.00497025</td><td>Female</td></tr><tr><th>22</th><td>46820</td><td>1</td><td>1</td><td>0.143298</td><td>0.856702</td><td>Male</td></tr><tr><th>23</th><td>147</td><td>0</td><td>0</td><td>0.994078</td><td>0.00592248</td><td>Female</td></tr><tr><th>24</th><td>21063</td><td>1</td><td>1</td><td>0.372932</td><td>0.627068</td><td>Male</td></tr><tr><th>25</th><td>20139</td><td>0</td><td>0</td><td>0.926061</td><td>0.0739391</td><td>Female</td></tr><tr><th>26</th><td>23288</td><td>0</td><td>0</td><td>0.726455</td><td>0.273545</td><td>Male</td></tr><tr><th>27</th><td>18957</td><td>0</td><td>0</td><td>0.786535</td><td>0.213465</td><td>Female</td></tr><tr><th>28</th><td>31256</td><td>1</td><td>0</td><td>0.64935</td><td>0.35065</td><td>Male</td></tr><tr><th>29</th><td>29642</td><td>1</td><td>0</td><td>0.65637</td><td>0.34363</td><td>Male</td></tr><tr><th>30</th><td>23638</td><td>0</td><td>0</td><td>0.57032</td><td>0.42968</td><td>Female</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& Column1 & target & prediction & class\\_0\\_prob & class\\_1\\_prob & gender\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Float64 & Float64 & String\\\\\n",
       "\t\\hline\n",
       "\t1 & 7762 & 0 & 0 & 0.995306 & 0.00469379 & Male \\\\\n",
       "\t2 & 23881 & 0 & 0 & 0.998258 & 0.00174155 & Female \\\\\n",
       "\t3 & 30507 & 0 & 0 & 0.996647 & 0.00335252 & Male \\\\\n",
       "\t4 & 28911 & 0 & 0 & 0.995548 & 0.0044521 & Female \\\\\n",
       "\t5 & 19484 & 0 & 0 & 0.972979 & 0.027021 & Male \\\\\n",
       "\t6 & 43031 & 0 & 0 & 0.587811 & 0.412189 & Male \\\\\n",
       "\t7 & 28188 & 0 & 0 & 0.758536 & 0.241464 & Male \\\\\n",
       "\t8 & 12761 & 0 & 0 & 0.896304 & 0.103696 & Male \\\\\n",
       "\t9 & 40834 & 0 & 0 & 0.713358 & 0.286642 & Female \\\\\n",
       "\t10 & 27875 & 0 & 0 & 0.702419 & 0.297581 & Female \\\\\n",
       "\t11 & 1276 & 1 & 1 & 0.157644 & 0.842356 & Male \\\\\n",
       "\t12 & 22608 & 1 & 0 & 0.690165 & 0.309835 & Male \\\\\n",
       "\t13 & 36230 & 0 & 0 & 0.984482 & 0.0155177 & Female \\\\\n",
       "\t14 & 13398 & 0 & 0 & 0.696728 & 0.303272 & Male \\\\\n",
       "\t15 & 43536 & 0 & 0 & 0.96563 & 0.0343704 & Male \\\\\n",
       "\t16 & 18627 & 0 & 0 & 0.726404 & 0.273596 & Male \\\\\n",
       "\t17 & 38424 & 0 & 0 & 0.653893 & 0.346107 & Male \\\\\n",
       "\t18 & 35505 & 1 & 1 & 0.3035 & 0.6965 & Male \\\\\n",
       "\t19 & 2372 & 0 & 0 & 0.760428 & 0.239572 & Male \\\\\n",
       "\t20 & 3375 & 0 & 0 & 0.70922 & 0.29078 & Male \\\\\n",
       "\t21 & 28331 & 0 & 0 & 0.99503 & 0.00497025 & Female \\\\\n",
       "\t22 & 46820 & 1 & 1 & 0.143298 & 0.856702 & Male \\\\\n",
       "\t23 & 147 & 0 & 0 & 0.994078 & 0.00592248 & Female \\\\\n",
       "\t24 & 21063 & 1 & 1 & 0.372932 & 0.627068 & Male \\\\\n",
       "\t25 & 20139 & 0 & 0 & 0.926061 & 0.0739391 & Female \\\\\n",
       "\t26 & 23288 & 0 & 0 & 0.726455 & 0.273545 & Male \\\\\n",
       "\t27 & 18957 & 0 & 0 & 0.786535 & 0.213465 & Female \\\\\n",
       "\t28 & 31256 & 1 & 0 & 0.64935 & 0.35065 & Male \\\\\n",
       "\t29 & 29642 & 1 & 0 & 0.65637 & 0.34363 & Male \\\\\n",
       "\t30 & 23638 & 0 & 0 & 0.57032 & 0.42968 & Female \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m14653×6 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m target \u001b[0m\u001b[1m prediction \u001b[0m\u001b[1m class_0_prob \u001b[0m\u001b[1m class_1_prob \u001b[0m\u001b[1m gender \u001b[0m\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Int64   \u001b[0m\u001b[90m Int64  \u001b[0m\u001b[90m Int64      \u001b[0m\u001b[90m Float64      \u001b[0m\u001b[90m Float64      \u001b[0m\u001b[90m String \u001b[0m\n",
       "───────┼─────────────────────────────────────────────────────────────────\n",
       "     1 │    7762       0           0      0.995306    0.00469379  Male\n",
       "     2 │   23881       0           0      0.998258    0.00174155  Female\n",
       "     3 │   30507       0           0      0.996647    0.00335252  Male\n",
       "     4 │   28911       0           0      0.995548    0.0044521   Female\n",
       "     5 │   19484       0           0      0.972979    0.027021    Male\n",
       "     6 │   43031       0           0      0.587811    0.412189    Male\n",
       "     7 │   28188       0           0      0.758536    0.241464    Male\n",
       "     8 │   12761       0           0      0.896304    0.103696    Male\n",
       "     9 │   40834       0           0      0.713358    0.286642    Female\n",
       "    10 │   27875       0           0      0.702419    0.297581    Female\n",
       "    11 │    1276       1           1      0.157644    0.842356    Male\n",
       "   ⋮   │    ⋮       ⋮         ⋮            ⋮             ⋮          ⋮\n",
       " 14644 │   26787       0           0      0.697204    0.302796    Male\n",
       " 14645 │   12168       0           0      0.98324     0.0167596   Female\n",
       " 14646 │   16224       0           0      0.944285    0.0557149   Male\n",
       " 14647 │   30925       0           0      0.974979    0.0250208   Male\n",
       " 14648 │     697       1           1      0.143737    0.856263    Male\n",
       " 14649 │   15938       0           0      0.570326    0.429674    Male\n",
       " 14650 │   27828       0           0      0.995557    0.00444253  Female\n",
       " 14651 │   28449       0           0      0.972625    0.0273752   Female\n",
       " 14652 │    5647       0           0      0.974525    0.0254755   Male\n",
       " 14653 │   27058       0           0      0.807813    0.192187    Female\n",
       "\u001b[36m                                                       14632 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = CSV.read(\"parsed_dataset/biased_results.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Statistical Parity Difference\n",
    "The bias metric measures the difference in the favorable outcome of two groups of population, one privileged and another unprivileged.\n",
    "\n",
    "The idea is simple: group up two groups of people by a specific trait (gender, race, location, etc) and calculate their class conditional probability of getting a favorable outcome.\n",
    "\n",
    "In our example of the Adult income dataset, the favorable outcome will be having >50K income and the unprivileged group will be female.\n",
    "\n",
    "The probability equation can be formulated as: \n",
    "$$\n",
    "\\text{Statistical Parity Difference} = P( \\text {favorable outcome} | \\text{unprivileged}) - P( \\text{favorable outcome} | \\text{privileged})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "male_df = filter(row -> row.gender == \"Male\", df);\n",
    "female_df = filter(row -> row.gender == \"Female\", df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference between female and male in terms of income: -0.19113"
     ]
    }
   ],
   "source": [
    "male_favorable_prob = sum(male_df.target) / nrow(male_df);\n",
    "female_favorable_prob = sum(female_df.target) / nrow(female_df);\n",
    "print(\"Statistical Parity Difference between female and male in terms of income: \", round(female_favorable_prob - male_favorable_prob, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Interpretation:\n",
    "- Ideal value = 0 \n",
    "- Usually values between -0.1 and 0.1 is considered fair\n",
    "- Measures which group is favored based on their probability of getting a good outcome (>50K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Equal Opportunity Difference\n",
    "This metric computes the difference between the true positive rates of unprivileged and privileged groups. \n",
    "Note that true positive for group $A$ is defined as \n",
    "$$\n",
    "\\frac{P(TP|group=A)}{P(P|group=A)}\n",
    "$$\n",
    "\n",
    "For instance, if we want to find out the true positive rate of female in terms of our classification earlier in the Python notebook:\n",
    "- filter the dataframe to include only female entries\n",
    "- in those entries, count the number of rows where label = prediction = 1\n",
    "- now count the number of actual positives, where the only condition is label = 1\n",
    "- finally, we divide the two to obtain the *true positive rate*\n",
    "\n",
    "$$\n",
    "\\frac{P(TP|group=F)}{P(P|group=F)} - \\frac{P(TP|group=M)}{P(P|group=M)}\n",
    "$$\n",
    "where $F$ = female and $M$ = male\n",
    "\n",
    "Interpretation of the metric\n",
    "- The ideal outcome will be no difference (i.e. = 0), which means both groups have the same opporunity to get positive result. In our case that means both female and male have an equal chance at being classified as having above 50K income.\n",
    "- values $<0$ means the model / system is biased towards benefitting the privileged group\n",
    "- values $>0$ means the model / system is biased towards benefitting the unprivileged group\n",
    "- Typcially, an acceptable (fair) range of the difference is $[-0.1, 0.1]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6038808966209435"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_positive_male = size(filter(row -> row.target == 1, male_df))[1];\n",
    "true_positive_male = size(filter(row -> row.target == 1 && row.prediction == 1, male_df))[1];\n",
    "\n",
    "tp_rate_male = true_positive_male / actual_positive_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4900900900900901"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_positive_female = size(filter(row -> row.target == 1, female_df))[1];\n",
    "true_positive_female = size(filter(row -> row.target == 1 && row.prediction == 1, female_df))[1];\n",
    "\n",
    "tp_rate_female = true_positive_female / actual_positive_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal Opportunity Difference between female and male is: -0.11379"
     ]
    }
   ],
   "source": [
    "# calculate the difference\n",
    "print(\"Equal Opportunity Difference between female and male is: \", round(tp_rate_female - tp_rate_male, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How do we explain this result? \n",
    "- Female has less opportunity compared to male when we use our classifier to classify income\n",
    "- The model classifies the positivity better in men compared to female.\n",
    "- In other words, if the model is in charge of admitting students or selecting new employees, it is more likely that the model makes a mistake on female applicants. \n",
    "- Ideally, we want the model to perform the same under different genders (or other protected features). \n",
    "\n",
    "## Average Odds Difference \n",
    "\n",
    "Another metric that is closely related to the equal opportunity difference is the average odds difference. Average odds difference is defined as the average difference of the false positive rate and true positive rate between unprivileged and privileged groups.\n",
    "This might be a little bit confusing, so let's try to write it in math\n",
    "\n",
    "$$\n",
    "{[P(FP| \\text{unprivileged}) -P(FP|  \\text{privileged})] + [ P(TP | \\text{unprivileged})- P(TP |  \\text{privileged})]} / 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09703028521023228"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_negative_male = size(filter(row -> row.target == 0, male_df))[1]; # negatives\n",
    "fp_male = size(filter(row -> row.target == 0 && row.prediction == 1, male_df))[1]; # false positive\n",
    "\n",
    "fp_rate_male = fp_male / actual_negative_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0162526120269329"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_negative_female = size(filter(row -> row.target == 0, female_df))[1];\n",
    "fp_female = size(filter(row -> row.target == 0 && row.prediction == 1, female_df))[1];\n",
    "\n",
    "fp_rate_female = fp_female / actual_negative_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.08077767318329938\n",
      "-0.11379080653085338\n"
     ]
    }
   ],
   "source": [
    "println(fp_rate_female - fp_rate_male)\n",
    "println(tp_rate_female - tp_rate_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Odds Difference between female and male is: -0.09728"
     ]
    }
   ],
   "source": [
    "print(\"Average Odds Difference between female and male is: \", round(((fp_rate_female - fp_rate_male) + (tp_rate_female - tp_rate_male))/2, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Again, let's try to understand what is happening: \n",
    "- The false positive rate of male is significantly higher compared to female's.\n",
    "- Implies that male is more likely to be misclassified in the >50K category while female's result is accurate\n",
    "- Ideally, we want the false positive rate to be the same to show that there is no bias\n",
    "- Intuitively: how much the unprivileged class is likely to be classified as positive (50>K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Now, let's head back to Python to see how we can rectify these biases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7",
   "env": {
    "JULIA_DEPOT_PATH": "/home/user/.julia/:/ext/julia/depot/",
    "JULIA_PROJECT": "/home/user/.julia/environment/v1.7"
   },
   "language": "julia",
   "metadata": {
    "cocalc": {
     "description": "The Julia Programming Language",
     "priority": 10,
     "url": "https://julialang.org/"
    }
   },
   "name": "julia-1.7",
   "resource_dir": "/ext/jupyter/kernels/julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
